{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer,MinMaxScaler,LabelEncoder,OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOY = pd.to_datetime(datetime.date(2021,6,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B     148\n",
       "A     132\n",
       "BQ    131\n",
       "C     131\n",
       "DA    128\n",
       "O     126\n",
       "BB    124\n",
       "AA    123\n",
       "BC    121\n",
       "AC    119\n",
       "RE    116\n",
       "Name: Insurance_company, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Insurance_company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_path</th>\n",
       "      <th>Insurance_company</th>\n",
       "      <th>Cost_of_vehicle</th>\n",
       "      <th>Min_coverage</th>\n",
       "      <th>Expiry_date</th>\n",
       "      <th>Max_coverage</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_4513976.jpg</td>\n",
       "      <td>BQ</td>\n",
       "      <td>41500.0</td>\n",
       "      <td>1037.5</td>\n",
       "      <td>2026-12-03</td>\n",
       "      <td>36142.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_7764995.jpg</td>\n",
       "      <td>BQ</td>\n",
       "      <td>50700.0</td>\n",
       "      <td>1267.5</td>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>12753.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_451308.jpg</td>\n",
       "      <td>A</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>1237.5</td>\n",
       "      <td>2022-08-11</td>\n",
       "      <td>43102.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_7768372.jpg</td>\n",
       "      <td>A</td>\n",
       "      <td>33500.0</td>\n",
       "      <td>837.5</td>\n",
       "      <td>2022-08-02</td>\n",
       "      <td>8453.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_7765274.jpg</td>\n",
       "      <td>AC</td>\n",
       "      <td>27600.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>2026-05-01</td>\n",
       "      <td>6978.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8849.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image_path Insurance_company  Cost_of_vehicle  Min_coverage  \\\n",
       "0  img_4513976.jpg                BQ          41500.0        1037.5   \n",
       "1  img_7764995.jpg                BQ          50700.0        1267.5   \n",
       "2   img_451308.jpg                 A          49500.0        1237.5   \n",
       "3  img_7768372.jpg                 A          33500.0         837.5   \n",
       "4  img_7765274.jpg                AC          27600.0         690.0   \n",
       "\n",
       "  Expiry_date  Max_coverage  Condition  Amount  \n",
       "0  2026-12-03      36142.68          0     0.0  \n",
       "1  2025-07-10      12753.00          1  6194.0  \n",
       "2  2022-08-11      43102.68          0     0.0  \n",
       "3  2022-08-02       8453.00          1  7699.0  \n",
       "4  2026-05-01       6978.00          1  8849.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(df[['Insurance_company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesa_atributos(input_path):\n",
    "    df=pd.read_csv(input_path)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop(columns=['Condition'])\n",
    "    df = df[(df['Amount']<=df['Cost_of_vehicle'])]\n",
    "    \n",
    "    df['Expiry_date']=pd.to_datetime(df['Expiry_date'])\n",
    "    df['dias_pendientes_cobertura']=(df['Expiry_date']-HOY).dt.days.astype(int)\n",
    "    df['relative_amount'] = df['Amount']/df['Cost_of_vehicle']\n",
    "    df['relative_max_cov'] = df['Max_coverage']/df['Cost_of_vehicle']\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns=['Min_coverage','Expiry_date','Max_coverage','Amount'])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    X = df.drop(columns=['relative_amount'])\n",
    "    y = df['relative_amount']\n",
    "    \n",
    "    X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.25, random_state=777)\n",
    "    \n",
    "    \n",
    "    X_train_num = X_train[['Cost_of_vehicle','dias_pendientes_cobertura']]\n",
    "    X_val_num = X_val[['Cost_of_vehicle','dias_pendientes_cobertura']]\n",
    "    \n",
    "    X_train_num_scaled = pd.DataFrame(scaler.fit_transform(X_train_num), \n",
    "                                      columns=X_train_num.columns,\n",
    "                                      index=X_train.index)\n",
    "    \n",
    "    X_val_num_scaled = pd.DataFrame(scaler.transform(X_val_num), \n",
    "                                      columns=X_val_num.columns,\n",
    "                                       index=X_val.index)\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    \n",
    "    X_train_encoded = pd.DataFrame(lb.fit_transform(X_train['Insurance_company']),\n",
    "                                    columns=lb.classes_,\n",
    "                                   index=X_train.index)\n",
    "    \n",
    "    X_val_encoded = pd.DataFrame(lb.transform(X_val['Insurance_company']),\n",
    "                                    columns=lb.classes_,\n",
    "                                   index=X_val.index)\n",
    "    \n",
    "    \n",
    "    X_train_feat = pd.concat([X_train['relative_max_cov'],X_train_num_scaled,X_train_encoded],axis=1)\n",
    "    X_val_feat = pd.concat([X_val['relative_max_cov'],X_val_num_scaled,X_val_encoded],axis=1)\n",
    "    \n",
    "    X_train_im = X_train['Image_path']\n",
    "    X_val_im = X_val['Image_path']\n",
    "    \n",
    "    \n",
    "    return X_train_feat,X_train_im, X_val_feat,X_val_im,y_train,y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat,X_train_im, X_val_feat,X_val_im,y_train,y_val = preprocesa_atributos('train_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisiera inspeccionar imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ni=1 \\n\\nfor indice in X_train_im.index:\\n\\n    img = cv2.imread('trainImages/'+X_train_im[indice])\\n    trans_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\\n    plt.imshow(trans_img)\\n    plt.title('foto '+str(i)+':'+str(y_train[indice]))\\n    plt.show()\\n    i+=1\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "i=1 \n",
    "\n",
    "for indice in X_train_im.index:\n",
    "\n",
    "    img = cv2.imread('trainImages/'+X_train_im[indice])\n",
    "    trans_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.imshow(trans_img)\n",
    "    plt.title('foto '+str(i)+':'+str(y_train[indice]))\n",
    "    plt.show()\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_imagenes(serie_fotos, inputPath):\n",
    "    \n",
    "    nuevas_imagenes = []\n",
    "    for foto in serie_fotos:\n",
    "        imagen = cv2.imread(inputPath+'/'+foto)\n",
    "        imagen = cv2.resize(imagen, (128, 128))\n",
    "        nuevas_imagenes.append(imagen)\n",
    "    return np.array(nuevas_imagenes)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(carga_imagenes(X_val_im,'trainImages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325, 14)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "# initialize the input shape and channel dimension, assuming\n",
    "# TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "    # if this is the first CONV layer then set the input\n",
    "    # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "\n",
    "    # CONV => RELU => BN => POOL\n",
    "    x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenes_train = carga_imagenes(X_train_im,'trainImages')\n",
    "imagenes_val = carga_imagenes(X_val_im,'trainImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(X_train_feat.shape[1], regress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = create_cnn(128, 128, 3, regress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = concatenate([mlp.output, cnn.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggoni/opt/anaconda3/envs/dl/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 01:02:26.054627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "122/122 [==============================] - 16s 123ms/step - loss: 31520784.0000 - val_loss: 2529709.7500\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 15s 120ms/step - loss: 16405226.0000 - val_loss: 5406155.0000\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 14s 118ms/step - loss: 14240994.0000 - val_loss: 4377897.0000\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 15s 119ms/step - loss: 8712942.0000 - val_loss: 2850682.5000\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 16s 131ms/step - loss: 5322680.0000 - val_loss: 2421362.2500\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 15s 122ms/step - loss: 6978489.5000 - val_loss: 263861.7188\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 16s 134ms/step - loss: 3124890.5000 - val_loss: 1370651.3750\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 16s 129ms/step - loss: 2225047.0000 - val_loss: 164923.1562\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 16s 127ms/step - loss: 2101711.2500 - val_loss: 230655.9688\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 15s 127ms/step - loss: 991012.3125 - val_loss: 312915.3125\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 15s 126ms/step - loss: 679798.1250 - val_loss: 468889.0938\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 15s 121ms/step - loss: 255624.2188 - val_loss: 14967.8105\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 15s 123ms/step - loss: 347547.8438 - val_loss: 2859.5667\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 15s 123ms/step - loss: 537765.9375 - val_loss: 123916.9453\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 1251037.6250 - val_loss: 86135.5938\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 15s 121ms/step - loss: 229116.1562 - val_loss: 4619.3335\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 14s 119ms/step - loss: 81410.3125 - val_loss: 12891.5859\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 16s 130ms/step - loss: 349375.6562 - val_loss: 37997.2188\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 16s 133ms/step - loss: 35688.7344 - val_loss: 7031.1646\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 17s 137ms/step - loss: 68212.4375 - val_loss: 77477.4141\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 16s 133ms/step - loss: 74392.1641 - val_loss: 30919.3867\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 17s 135ms/step - loss: 31083.3477 - val_loss: 17622.6855\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 18s 145ms/step - loss: 37343.4883 - val_loss: 130812.7031\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 16s 128ms/step - loss: 51772.8828 - val_loss: 4286.6367\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 15s 121ms/step - loss: 87547.3281 - val_loss: 84104.3984\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 19845.3926 - val_loss: 2224.9326\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - 16s 132ms/step - loss: 48215.7656 - val_loss: 204.4937\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 15s 123ms/step - loss: 97621.7812 - val_loss: 10280.1250\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 15s 127ms/step - loss: 94607.1172 - val_loss: 1426.2202\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 16s 132ms/step - loss: 27427.4961 - val_loss: 79069.6016\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 16s 135ms/step - loss: 62015.4609 - val_loss: 34916.4688\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 15s 126ms/step - loss: 33061.6406 - val_loss: 22174.9883\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 16s 127ms/step - loss: 30192.8105 - val_loss: 63982.0508\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 18s 144ms/step - loss: 35545.3008 - val_loss: 56260.9336\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 18s 146ms/step - loss: 26599.5801 - val_loss: 99924.5781\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 16s 132ms/step - loss: 66367.7188 - val_loss: 28548.6211\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 16s 131ms/step - loss: 39885.2695 - val_loss: 111793.9688\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 17s 137ms/step - loss: 66906.9766 - val_loss: 53442.3398\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 16s 132ms/step - loss: 203423.3438 - val_loss: 83236.1016\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 16s 135ms/step - loss: 47337.9766 - val_loss: 11090.0820\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 18s 147ms/step - loss: 39162.8203 - val_loss: 10060.0732\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 16s 135ms/step - loss: 32445.4375 - val_loss: 23656.8262\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 15s 125ms/step - loss: 16110.4727 - val_loss: 92643.8984\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 15s 125ms/step - loss: 44612.7227 - val_loss: 6224.8447\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 15s 126ms/step - loss: 17546.6211 - val_loss: 13035.5244\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 15s 125ms/step - loss: 72177.9531 - val_loss: 73806.1953\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 15s 125ms/step - loss: 48745.9648 - val_loss: 28852.9531\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 15s 126ms/step - loss: 74940.1953 - val_loss: 24997.9219\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 16s 127ms/step - loss: 36445.6250 - val_loss: 20001.7871\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 15s 123ms/step - loss: 20080.9980 - val_loss: 11460.7988\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 57896.1641 - val_loss: 36552.4531\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 32265.5859 - val_loss: 79862.5078\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 15s 126ms/step - loss: 48026.6562 - val_loss: 15878.6416\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 28659.0742 - val_loss: 34275.7617\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 26375.9844 - val_loss: 6585.0571\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 15s 127ms/step - loss: 36933.9062 - val_loss: 31741.3086\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 14s 119ms/step - loss: 23815.2930 - val_loss: 41763.3086\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 15s 127ms/step - loss: 29961.0098 - val_loss: 32405.1230\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 42002.7734 - val_loss: 36693.9570\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 15s 119ms/step - loss: 50956.4648 - val_loss: 33635.0117\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 15s 120ms/step - loss: 18054.4473 - val_loss: 26450.3691\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 14s 119ms/step - loss: 39149.3750 - val_loss: 3788.8201\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 14s 119ms/step - loss: 42230.8672 - val_loss: 86288.5859\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 15s 120ms/step - loss: 39452.4023 - val_loss: 64023.4961\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 15s 119ms/step - loss: 43664.7305 - val_loss: 22084.5605\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 15s 121ms/step - loss: 34376.3984 - val_loss: 7692.7632\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 15s 121ms/step - loss: 29207.4434 - val_loss: 1432.5486\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 15s 120ms/step - loss: 26756.7852 - val_loss: 9869.2383\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 15s 122ms/step - loss: 80432.9609 - val_loss: 4981.4390\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 15s 124ms/step - loss: 30203.8477 - val_loss: 55714.3086\n",
      "Epoch 71/200\n",
      " 56/122 [============>.................] - ETA: 8s - loss: 12747.0918"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "    x=[X_train_feat.values, imagenes_train], y=y_train.values,\n",
    "    validation_data=([X_val_feat.values, imagenes_val], y_val.values),\n",
    "    epochs=200, batch_size=8)\n",
    "\n",
    "# make predictions on the testing data\n",
    "#print(\"[INFO] predicting house prices...\")\n",
    "#preds = model.predict([testAttrX, testImagesX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996     img_4538327.jpg\n",
       " 1132     img_451799.jpg\n",
       " 720     img_4515734.jpg\n",
       " 209     img_4518363.jpg\n",
       " 438     img_4634857.jpg\n",
       "              ...       \n",
       " 695     img_7768762.jpg\n",
       " 83      img_4538608.jpg\n",
       " 1013    img_4514629.jpg\n",
       " 886     img_7766226.jpg\n",
       " 119     img_4638780.jpg\n",
       " Name: Image_path, Length: 975, dtype: object,\n",
       " 'trainImages')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0080863 , 0.        , 0.00879699, 0.16507463, 0.08255528,\n",
       "       0.        , 0.08794872, 0.        , 0.0132287 , 0.05426104,\n",
       "       0.1478211 , 0.20750988, 0.23074074, 0.13422983, 0.14958084,\n",
       "       0.16285714, 0.07604651, 0.17280936, 0.07137014, 0.10022774,\n",
       "       0.24662602, 0.21387097, 0.02862832, 0.34282759, 0.08180467,\n",
       "       0.        , 0.12256522, 0.0129653 , 0.166691  , 0.12137555,\n",
       "       0.10124294, 0.19170799, 0.10311213, 0.02390313, 0.25870968,\n",
       "       0.17178922, 0.11      , 0.01464135, 0.19801843, 0.14967914,\n",
       "       0.25170455, 0.15461126, 0.14420613, 0.01171488, 0.06153846,\n",
       "       0.02138318, 0.11220561, 0.02141876, 0.2347482 , 0.02484848,\n",
       "       0.00982759, 0.17596354, 0.08935733, 0.06567982, 0.06058824,\n",
       "       0.17432373, 0.15747541, 0.28468085, 0.10927492, 0.1763986 ,\n",
       "       0.1865252 , 0.11592476, 0.22330396, 0.12216963, 0.10065217,\n",
       "       0.16134804, 0.09046371, 0.06621032, 0.08930288, 0.18332203,\n",
       "       0.00876448, 0.1114717 , 0.16874652, 0.20843949, 0.03800937,\n",
       "       0.19205607, 0.14468208, 0.16471883, 0.15625344, 0.15466899,\n",
       "       0.08028721, 0.1802008 , 0.13043478, 0.30146875, 0.11010684,\n",
       "       0.34930041, 0.1759335 , 0.10691358, 0.09524138, 0.08062264,\n",
       "       0.20752475, 0.08955466, 0.07526316, 0.12228037, 0.14800664,\n",
       "       0.12626667, 0.0737594 , 0.19833708, 0.129525  , 0.19438163,\n",
       "       0.1054026 , 0.26507426, 0.19327586, 0.10588803, 0.        ,\n",
       "       0.15881783, 0.0185446 , 0.01460938, 0.16247423, 0.08368932,\n",
       "       0.16466357, 0.08097328, 0.09776618, 0.13953947, 0.18175824,\n",
       "       0.        , 0.00888021, 0.26242424, 0.05590389, 0.07578261,\n",
       "       0.15063889, 0.01226306, 0.02278846, 0.09410256, 0.06212454,\n",
       "       0.05161479, 0.02653933, 0.23266129, 0.17128019, 0.25485477,\n",
       "       0.18433628, 0.13350617, 0.        , 0.0937963 , 0.13671123,\n",
       "       0.22078292, 0.37568627, 0.1960119 , 0.02289231, 0.08794118,\n",
       "       0.01378723, 0.07119816, 0.01748889, 0.01091358, 0.09449024,\n",
       "       0.01272917, 0.13673469, 0.01858744, 0.03825792, 0.01452333,\n",
       "       0.        , 0.18152542, 0.25513966, 0.0873506 , 0.03020408,\n",
       "       0.01410042, 0.43125541, 0.0535034 , 0.05045817, 0.16895349,\n",
       "       0.21679181, 0.23095563, 0.11954887, 0.15403628, 0.37978261,\n",
       "       0.01714286, 0.10759596, 0.01360656, 0.24511278, 0.04828512,\n",
       "       0.20217532, 0.10805046, 0.01317507, 0.14522807, 0.10505236,\n",
       "       0.        , 0.16608   , 0.18945312, 0.07879612, 0.1973029 ,\n",
       "       0.        , 0.08728489, 0.1411    , 0.22491961, 0.35371094,\n",
       "       0.14662651, 0.01468493, 0.12765432, 0.        , 0.41705628,\n",
       "       0.28546392, 0.09742004, 0.15785249, 0.15205589, 0.20472376,\n",
       "       0.13941818, 0.1947448 , 0.01366197, 0.14928854, 0.16657596,\n",
       "       0.15859275, 0.06832677, 0.17267677, 0.13873786, 0.26363889,\n",
       "       0.18022495, 0.19512048, 0.11159664, 0.02720339, 0.20861538,\n",
       "       0.        , 0.1568559 , 0.00613208, 0.08121704, 0.201875  ,\n",
       "       0.1073913 , 0.22853061, 0.01465704, 0.03903475, 0.02886297,\n",
       "       0.16968858, 0.11518272, 0.2016    , 0.22250794, 0.16272727,\n",
       "       0.06932203, 0.04705405, 0.19117089, 0.02676471, 0.26003831,\n",
       "       0.04002404, 0.13135484, 0.        , 0.1663587 , 0.19790076,\n",
       "       0.07632022, 0.0855814 , 0.00853428, 0.47717842, 0.09564516,\n",
       "       0.        , 0.13620991, 0.11202703, 0.        , 0.10456332,\n",
       "       0.26746914, 0.        , 0.19272997, 0.02711806, 0.27292683,\n",
       "       0.18049603, 0.        , 0.02058442, 0.        , 0.0124594 ,\n",
       "       0.10875912, 0.12353808, 0.12597656, 0.11516588, 0.01911315,\n",
       "       0.02288828, 0.29034632, 0.13758865, 0.08138568, 0.14107143,\n",
       "       0.12738854, 0.07285992, 0.17349862, 0.04830078, 0.        ,\n",
       "       0.14176849, 0.16360294, 0.02223108, 0.11538071, 0.01805252,\n",
       "       0.09307692, 0.06524675, 0.13481855, 0.        , 0.01001887,\n",
       "       0.09446623, 0.14860113, 0.09307368, 0.15395604, 0.01368217,\n",
       "       0.14115304, 0.07408624, 0.13427692, 0.15930788, 0.07837209,\n",
       "       0.09042403, 0.19223301, 0.19409396, 0.12051095, 0.0847    ,\n",
       "       0.04755474, 0.15633484, 0.11201465, 0.01355649, 0.10273885,\n",
       "       0.12722714, 0.20118156, 0.0104947 , 0.15216146, 0.01238956,\n",
       "       0.11834532, 0.07049462, 0.10118785, 0.14254545, 0.39771084,\n",
       "       0.11955882, 0.09453271, 0.25718876, 0.11944086, 0.18941304,\n",
       "       0.07561702, 0.16213376, 0.17372294, 0.08268542, 0.01707921,\n",
       "       0.        , 0.19434286, 0.123382  , 0.16960474, 0.19127273,\n",
       "       0.12068807, 0.13530172, 0.14804408, 0.06149398, 0.01861432,\n",
       "       0.21223729, 0.3363745 , 0.15532394, 0.55760563, 0.14727041,\n",
       "       0.07907407, 0.23204878, 0.23703297, 0.02057613, 0.09124424,\n",
       "       0.02752066, 0.12236923, 0.10392857, 0.08414784, 0.06331395,\n",
       "       0.1911039 , 0.01507576, 0.14374723, 0.13323129, 0.00746439,\n",
       "       0.16342857, 0.15680808, 0.14225309, 0.24452282, 0.24910714,\n",
       "       0.02341463, 0.15442516, 0.01704981, 0.12514412, 0.19602906,\n",
       "       0.13710801, 0.10456204, 0.38054745, 0.        , 0.12558201,\n",
       "       0.14794582, 0.1729199 , 0.14481481, 0.17595133, 0.19742991,\n",
       "       0.10069328, 0.1663871 , 0.12329218, 0.00620773, 0.17631429,\n",
       "       0.14816367, 0.32061594, 0.0565    , 0.1501476 , 0.20953431,\n",
       "       0.08747012, 0.08336484, 0.23148936, 0.19498113, 0.12449275,\n",
       "       0.17169492, 0.13651064, 0.09571429, 0.03081146, 0.06260456,\n",
       "       0.09253444, 0.1698645 , 0.18260073, 0.19776398, 0.13874459,\n",
       "       0.04340824, 0.09004454, 0.06444444, 0.1289899 , 0.19682692,\n",
       "       0.02040373, 0.13631373, 0.02746324, 0.12347107, 0.03485106,\n",
       "       0.25907563, 0.00925926, 0.15798817, 0.00309735, 0.09170418,\n",
       "       0.02543417, 0.00729665, 0.01354212, 0.10898268, 0.17581731,\n",
       "       0.12479008, 0.2642623 , 0.10647917, 0.1189243 , 0.09165725,\n",
       "       0.18598394, 0.10602817, 0.20410256, 0.11383795, 0.0894246 ,\n",
       "       0.22562791, 0.1210728 , 0.16987705, 0.01228846, 0.16993631,\n",
       "       0.00347222, 0.0439    , 0.11137736, 0.1352164 , 0.28459227,\n",
       "       0.18653367, 0.17139364, 0.05932886, 0.02820059, 0.07089202,\n",
       "       0.15931889, 0.11983974, 0.16210084, 0.303625  , 0.14508159,\n",
       "       0.        , 0.41170833, 0.08817978, 0.30641577, 0.10640693,\n",
       "       0.1602027 , 0.03364   , 0.06341346, 0.20463007, 0.09493075,\n",
       "       0.11411417, 0.06853211, 0.0145509 , 0.01848921, 0.25967123,\n",
       "       0.13178313, 0.14943333, 0.11797546, 0.09175   , 0.        ,\n",
       "       0.05357853, 0.        , 0.        , 0.09316993, 0.16043046,\n",
       "       0.01833333, 0.19826613, 0.06509278, 0.0811007 , 0.20932476,\n",
       "       0.17945455, 0.12532143, 0.        , 0.26199324, 0.        ,\n",
       "       0.13300885, 0.10225581, 0.        , 0.17679666, 0.17327434,\n",
       "       0.18062626, 0.07694087, 0.05494062, 0.35604839, 0.14150579,\n",
       "       0.02      , 0.01270642, 0.01272331, 0.11090116, 0.14771795,\n",
       "       0.09230986, 0.13490775, 0.23376147, 0.17426471, 0.28720867,\n",
       "       0.33115132, 0.02817365, 0.0140367 , 0.        , 0.        ,\n",
       "       0.03302181, 0.07052863, 0.        , 0.10209068, 0.01845638,\n",
       "       0.11411523, 0.35257143, 0.20234657, 0.07530952, 0.01535484,\n",
       "       0.24333333, 0.29829352, 0.19174648, 0.01911877, 0.40167939,\n",
       "       0.3083871 , 0.01630542, 0.07311966, 0.18230616, 0.1979096 ,\n",
       "       0.15382353, 0.08653333, 0.10568182, 0.26571802, 0.06380682,\n",
       "       0.        , 0.18202934, 0.0942381 , 0.07240246, 0.10147945,\n",
       "       0.09216704, 0.14800937, 0.        , 0.09319892, 0.1415443 ,\n",
       "       0.18644491, 0.18707216, 0.02249042, 0.4189083 , 0.32897106,\n",
       "       0.11946237, 0.1397491 , 0.08135542, 0.00957393, 0.        ,\n",
       "       0.13501014, 0.12912065, 0.02066914, 0.        , 0.03269841,\n",
       "       0.07907029, 0.09130081, 0.10785855, 0.22406077, 0.11761134,\n",
       "       0.1522314 , 0.22390572, 0.0463834 , 0.00967054, 0.13661585,\n",
       "       0.01775424, 0.16609694, 0.02973684, 0.11031519, 0.06053079,\n",
       "       0.        , 0.09243421, 0.23741935, 0.11896552, 0.16385621,\n",
       "       0.        , 0.25481781, 0.13763359, 0.24677582, 0.01064655,\n",
       "       0.06767568, 0.15310219, 0.15329897, 0.01004024, 0.16315287,\n",
       "       0.14093333, 0.03057221, 0.29277372, 0.07543233, 0.15722433,\n",
       "       0.14187867, 0.10952741, 0.23383333, 0.18580994, 0.01391304,\n",
       "       0.16971545, 0.01531429, 0.35688797, 0.        , 0.17205776,\n",
       "       0.08373786, 0.12703557, 0.02413907, 0.15134529, 0.0921875 ,\n",
       "       0.08154717, 0.28544248, 0.14146982, 0.18730216, 0.15562249,\n",
       "       0.08838608, 0.        , 0.13384306, 0.06507431, 0.2365625 ,\n",
       "       0.06557769, 0.19331878, 0.11992   , 0.0192766 , 0.2964257 ,\n",
       "       0.07414317, 0.09843284, 0.2186692 , 0.15301235, 0.12979424,\n",
       "       0.12588636, 0.26768025, 0.10665072, 0.0021978 , 0.01546032,\n",
       "       0.10453947, 0.14988987, 0.14081851, 0.02614776, 0.13679208,\n",
       "       0.06240896, 0.01180272, 0.20682836, 0.02321429, 0.00541748,\n",
       "       0.00520309, 0.20634551, 0.335     , 0.01113546, 0.12373585,\n",
       "       0.        , 0.15241509, 0.11876364, 0.1376555 , 0.102     ,\n",
       "       0.        , 0.20506061, 0.1181383 , 0.2906734 , 0.16972028,\n",
       "       0.00815217, 0.21212625, 0.01069231, 0.01803493, 0.30325397,\n",
       "       0.11088477, 0.01243402, 0.11878788, 0.24565333, 0.17291444,\n",
       "       0.16235119, 0.25948276, 0.07811947, 0.1171746 , 0.13798077,\n",
       "       0.03477444, 0.20297436, 0.06533505, 0.25057692, 0.07089485,\n",
       "       0.05359551, 0.2185098 , 0.0737386 , 0.14735974, 0.19064   ,\n",
       "       0.        , 0.122     , 0.        , 0.10495522, 0.2       ,\n",
       "       0.07591549, 0.06497717, 0.        , 0.02380567, 0.07514469,\n",
       "       0.00655431, 0.08880562, 0.01001946, 0.02696581, 0.10831683,\n",
       "       0.06021645, 0.0339    , 0.17930362, 0.08237363, 0.29279245,\n",
       "       0.13882979, 0.        , 0.18895385, 0.00940503, 0.0237931 ,\n",
       "       0.02610966, 0.16249322, 0.27450593, 0.08977221, 0.01492228,\n",
       "       0.13822222, 0.04      , 0.11600583, 0.12198529, 0.1331    ,\n",
       "       0.22947598, 0.09256674, 0.06554415, 0.10661417, 0.11201681,\n",
       "       0.31224409, 0.        , 0.10722365, 0.12599359, 0.1429023 ,\n",
       "       0.11883333, 0.17330526, 0.10831373, 0.12005435, 0.21287554,\n",
       "       0.        , 0.1444086 , 0.19608974, 0.01639313, 0.19570968,\n",
       "       0.10355705, 0.27935135, 0.04938272, 0.0145283 , 0.11895238,\n",
       "       0.20470968, 0.07186147, 0.08103614, 0.        , 0.14004065,\n",
       "       0.02625   , 0.05442907, 0.13234657, 0.01781377, 0.08089701,\n",
       "       0.14418182, 0.        , 0.09567639, 0.19990798, 0.16479167,\n",
       "       0.19922764, 0.01263889, 0.13442266, 0.        , 0.05382066,\n",
       "       0.12995506, 0.13907489, 0.12681293, 0.        , 0.01762463,\n",
       "       0.00837696, 0.17054404, 0.01676606, 0.04690083, 0.11271845,\n",
       "       0.07017429, 0.08467811, 0.07824675, 0.01228205, 0.09768173,\n",
       "       0.        , 0.01608187, 0.12974937, 0.15444444, 0.09815951,\n",
       "       0.01837079, 0.08519022, 0.        , 0.27112628, 0.20575758,\n",
       "       0.14954918, 0.14470588, 0.05553785, 0.21366795, 0.12823529,\n",
       "       0.09465021, 0.        , 0.12757519, 0.1056746 , 0.14894928,\n",
       "       0.18025424, 0.08082192, 0.25358079, 0.00904398, 0.15957237,\n",
       "       0.03472   , 0.09441406, 0.11626638, 0.07326964, 0.00490196,\n",
       "       0.142     , 0.02390323, 0.14902381, 0.08619048, 0.08573427,\n",
       "       0.05333333, 0.214811  , 0.06224719, 0.00316923, 0.15403061,\n",
       "       0.14057307, 0.10653846, 0.16578313, 0.08344214, 0.01208889,\n",
       "       0.        , 0.01295337, 0.11914081, 0.00954802, 0.        ,\n",
       "       0.06699812, 0.02410029, 0.11727103, 0.27516923, 0.18614191,\n",
       "       0.32284672, 0.        , 0.03244713, 0.19346154, 0.12111905,\n",
       "       0.08765504, 0.12055556, 0.01171488, 0.07605081, 0.24740291,\n",
       "       0.11355224, 0.1067382 , 0.19746753, 0.12732759, 0.01092965,\n",
       "       0.18538961, 0.        , 0.01284444, 0.2       , 0.136     ,\n",
       "       0.1271875 , 0.205     , 0.        , 0.0028169 , 0.2010231 ,\n",
       "       0.06102515, 0.02107438, 0.        , 0.1161039 , 0.06589354,\n",
       "       0.32133588, 0.15258799, 0.10894578, 0.03154574, 0.08494949,\n",
       "       0.25123348, 0.0581749 , 0.23941476, 0.12741602, 0.        ,\n",
       "       0.        , 0.05284569, 0.16737037, 0.17039216, 0.11870518,\n",
       "       0.        , 0.03      , 0.01842912, 0.07660793, 0.13134529,\n",
       "       0.01296588, 0.08280093, 0.11672854, 0.17547297, 0.        ,\n",
       "       0.04618577, 0.08646635, 0.        , 0.01694444, 0.19291317,\n",
       "       0.        , 0.04826923, 0.20084559, 0.24770161, 0.16851124,\n",
       "       0.09420048, 0.09896617, 0.1155314 , 0.06111429, 0.16053004,\n",
       "       0.12796791, 0.03023622, 0.09468944, 0.        , 0.06446194,\n",
       "       0.26700965, 0.0694533 , 0.1887619 , 0.12130081, 0.08615578,\n",
       "       0.1009863 , 0.06693957, 0.20778667, 0.10445076, 0.24214286,\n",
       "       0.08491379, 0.08724638, 0.11328413, 0.01267223, 0.        ,\n",
       "       0.29857664, 0.13054326, 0.1529646 , 0.15684466, 0.01229703,\n",
       "       0.01250951, 0.1192    , 0.16227414, 0.18847107, 0.02509434,\n",
       "       0.01803175, 0.33632727, 0.13825436, 0.12013423, 0.15780702,\n",
       "       0.22232283, 0.01334579, 0.08845361, 0.09767081, 0.02925287,\n",
       "       0.2312    , 0.22503937, 0.15923567, 0.        , 0.1032491 ,\n",
       "       0.13868421, 0.        , 0.        , 0.21995305, 0.        ,\n",
       "       0.23867521, 0.01524876, 0.01513514, 0.14792642, 0.01396396,\n",
       "       0.02458753, 0.005     , 0.0347479 , 0.14081967, 0.12757485,\n",
       "       0.18470437, 0.08095142, 0.10882979, 0.13893891, 0.17769912,\n",
       "       0.09652422, 0.1674184 , 0.23711934, 0.1366065 , 0.17246575,\n",
       "       0.12282548, 0.15143969, 0.05732283, 0.13240404, 0.10162338])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
